{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b062fb0-03ec-4ca8-9067-0272560780c2",
   "metadata": {},
   "source": [
    "# 1. INTRODUCTION\n",
    "Welcome to the Jupyter Notebook on **Automated Data Processing with Python**. In this notebook, I will walk through the creation of a Python script designed to automate data processing tasks on various datasets. This includes calculating summary statistics, filtering data, generating visualizations, and saving the processed results. This script that can be adapted to different datasets but for demonstration purposes, we will use a dataset related to sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d256b-4379-4866-ba8d-7ee4a90f0dc0",
   "metadata": {},
   "source": [
    "# 2. IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd568e0-14fa-48a1-940b-fef6927be5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e44df-c139-46de-9c8e-e1c55a556b47",
   "metadata": {},
   "source": [
    "# 3. DATASET SELECTION\n",
    "I will start by loading the SalesData csv file using the pandas function *read_csv* and examining a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5011d-81b4-453a-b85e-c0abbbe42a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        print(f\"Data loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found. Please check the file path and try again.\")\n",
    "    \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: The file '{file_path}' is empty. Please provide a valid file with data.\")\n",
    "    \n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: The file '{file_path}' could not be parsed. Please check the file format and ensure it is a valid CSV file.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading the file: {e}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a48c80-e6bb-4397-b695-02420ca9722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "file_path = 'SalesData.csv'\n",
    "original_df = load_data(file_path)\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef6cf0-2273-44b0-bb4f-93d1dce0e2cf",
   "metadata": {},
   "source": [
    "# 4. DATA PREPARATION AND CLEANING\n",
    "In this section, I will prepare and clean the dataset to ensure it's in the best possible shape for analysis. This step is crucial for obtaining accurate and reliable insights from the data\n",
    "### - Create a Copy of the Data:\n",
    "This ensures that the original dataset remains unaltered\n",
    "### - Remove Duplicate Entries:\n",
    "Removing duplicates helps in ensuring that each record is unique and contributes accurately to the analysis.\n",
    "### - Handle Missing Values:\n",
    "Fill in missing values in the dataset. For numeric columns, I will use the mean of the column. For categorical columns, I will use the mode (most frequent value).\n",
    "### - Convert Date Columns to Datetime Format:\n",
    "Convert columns with 'Date' in their names to the datetime format to allows for accurate sorting, filtering, and analysis based on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a227a-002d-489f-bbfc-849a866ffe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    # Create a copy of the original DataFrame to work with\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Replace infinite values with NaN\n",
    "    df_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_copy.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df_copy.dropna(inplace=True)\n",
    "\n",
    "    # Fill in missing values\n",
    "    for column in df_copy.columns:\n",
    "        if df_copy[column].dtype == 'object':\n",
    "            # Fill missing values in categorical columns with mode\n",
    "            df_copy[column].fillna(df_copy[column].mode()[0], inplace=True)\n",
    "        else:\n",
    "            # Fill missing values in numeric columns with mean\n",
    "            df_copy[column].fillna(df_copy[column].mean(), inplace=True)\n",
    "\n",
    "    # Convert any column with 'Date' in its name to datetime format\n",
    "    for column in df_copy.columns:\n",
    "        if 'date' in column.lower():\n",
    "            df_copy[column] = pd.to_datetime(df_copy[column], errors='coerce')\n",
    "            # Remove the time part, retaining only the date\n",
    "            df_copy[column] = df_copy[column].dt.date\n",
    "\n",
    "    print(f\"Data cleaning completed: {df.shape[0] - df_copy.shape[0]} duplicates removed, missing values filled, and date columns converted.\")\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7bb42-dbb0-408a-933f-52db7325926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = prepare_data(original_df)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53874868-6682-433c-98d8-55d05c58af32",
   "metadata": {},
   "source": [
    "# 5. CALCULATING SUMMARY STATISTICS\n",
    "Next is to calculate some basic summary statistics, such as mean, median, and standard deviation, using pandas method *describe()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23836d22-3158-40b2-a147-dc208b248bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_statistics(df):\n",
    "    summary = df.describe()\n",
    "    print(\"Summary Statistics:\\n\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15e42b-b3ae-4b5a-ab3a-a40e42e759f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for the dataset\n",
    "summary_statistics = calculate_summary_statistics(cleaned_df)\n",
    "summary_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f9445-74d5-4233-8875-fb22ccddb88f",
   "metadata": {},
   "source": [
    "# 6. DATA FILTERING\n",
    "Now, I will introduce the functioality to filter the dataset based on specific criteria. For instance, we might want to filter the data for a specific region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d6d8e-dbba-4f8f-b3d1-08f2e147c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, column, value):\n",
    "    filtered_df = df[df[column] == value]\n",
    "    print(f\"Filtered Data by {column} = {value}:\\n\")\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17189c-42ee-4226-bd51-9d8d4fa78762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the data for a specific month\n",
    "filtered_df = filter_data(cleaned_df, 'Month', 9)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492461cf-da56-4c12-a7e4-f3c2aa45f8bd",
   "metadata": {},
   "source": [
    "# 7. GENERATING DATA VISUALIZATIONS\n",
    "I will now use Seaborn for visualizations, as it offers more aesthetically pleasing and complex visualizations compared to standard plotting methods. However, I'll also continue using Matplotlib for its additional flexibility and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83fca5-02a7-4366-98fc-b8019f864df2",
   "metadata": {},
   "source": [
    "### 7.1 Histogram\n",
    "A graphical representation of the distribution of numerical data. It shows the frequency of data points within specified ranges (bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e0e43-bc9d-4b7e-ab5d-2b0e93b8db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(df, column):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[column], kde=True, bins=10)\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b45a0-079b-42dd-8cd9-3648738533d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A histogram plot for the 'Sales' column to show the distribution of sales values across all transactions in the dataset. \n",
    "#This visualization helps us understand the frequency of different sales amounts.\n",
    "plot_histogram(cleaned_df, 'Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb551653-fc37-47e5-a5ce-47daaa542556",
   "metadata": {},
   "source": [
    "### 7.2 Bar Chart\n",
    "A bar chart is used to display and compare the total values of different categories. It shows the sum (or other aggregate function) of the numerical values for each category in the specified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6351c-46c3-4ac8-a26d-8f7f7cc9f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(df, category_col, value_col):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(x=category_col, y=value_col, data=df, estimator=sum, errorbar=None, palette='viridis')\n",
    "    plt.title(f'Total {value_col} by {category_col}')\n",
    "    plt.xlabel(category_col)\n",
    "    plt.ylabel(f'Total {value_col}')\n",
    "    plt.xticks(rotation=90)  # Rotate the x labels for better readability\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a8f33-14e6-4a71-9d0d-4e1c28cc9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bar chart for 'Product' vs. 'Sales' to showcases the total sales for each product. \n",
    "#By summing the sales amounts, we can easily see which products are the top performers and which are lagging.\n",
    "plot_bar_chart(cleaned_df, 'Product', 'Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc50890-ab0b-4e05-b22f-b572b6842fae",
   "metadata": {},
   "source": [
    "### 7.3 Pie Chart\n",
    "A pie chart is useful to show the proportion of different categories within a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c361f-ed90-4378-ba18-07a63baa78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(df, category_col, value_col, explode_index=0):\n",
    "    aggregated_data = df.groupby(category_col)[value_col].sum()\n",
    "    \n",
    "    # Create an explode array where one slice is popped out\n",
    "    explode = [0] * len(aggregated_data)\n",
    "    explode[explode_index] = 0.1\n",
    "    \n",
    "    # Plot pie chart\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(aggregated_data, labels=aggregated_data.index, autopct='%1.1f%%', \n",
    "            colors=sns.color_palette('pastel'), explode=explode)\n",
    "    plt.title(f'{value_col} Distribution by {category_col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b645d8-861b-41ed-9887-4ecb98343128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pie chart plot illustrates the distribution of total sales across different cities\n",
    "plot_pie_chart(cleaned_df, 'City', 'Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88460ed2-3209-401c-8f35-f29c974f9962",
   "metadata": {},
   "source": [
    "### 7.4 Scatter Plot\n",
    "A scatter plot helps to visualize the relationship between two numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9acccbb-b660-4b91-ac58-b2480b7475e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(df, x_col, y_col, hue_col=None):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    if hue_col:\n",
    "        sns.scatterplot(x=x_col, y=y_col, hue=hue_col, data=df)\n",
    "    else:\n",
    "        sns.scatterplot(x=x_col, y=y_col, data=df)\n",
    "    plt.title(f'{y_col} vs. {x_col}')\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    if hue_col:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5a4f5-f28c-4d10-879d-2f47bed725ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scatter plot depicts the relationship between price for each item and sales.\n",
    "plot_scatter(cleaned_df,'Price Each', 'Sales', 'Product')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec143f1-54ba-4c79-be19-8787771f877f",
   "metadata": {},
   "source": [
    "# 8. SAVING PROCESSED DATA\n",
    "Fuction that saves the processed data to new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d889bc-c972-4dd9-906e-3c0c8bedc88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(df, output_path):\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Processed data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6777ee4-94e0-4239-bca2-971aeaa21631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to a new file\n",
    "output_path = 'filtered_sales_data.csv'\n",
    "save_processed_data(cleaned_df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf451ec-7266-49dc-8373-713f097b0e7e",
   "metadata": {},
   "source": [
    "# 9. MAIN FUNCTION\n",
    "The main function ties everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a2f1d-2ef5-483e-ac72-ff0c05ef5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Process and analyze sales data.\")\n",
    "    parser.add_argument('file_path', type=str, help=\"Path to the input CSV file\")\n",
    "    parser.add_argument('output_path', type=str, help=\"Path to save the processed CSV file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "   #Add arguments for interactive functions\n",
    "    parser.add_argument('--filter_column', type=str, help=\"Column to filter on\")\n",
    "    parser.add_argument('--filter_value', type=str, help=\"Value to filter by\")\n",
    "    \n",
    "    parser.add_argument('--histogram_col', type=str, help=\"Column for histogram\")\n",
    "    parser.add_argument('--bar_category_col', type=str, help=\"Category column for bar chart\")\n",
    "    parser.add_argument('--bar_value_col', type=str, help=\"Value column for bar chart\")\n",
    "    \n",
    "    parser.add_argument('--pie_category_col', type=str, help=\"Category column for pie chart\")\n",
    "    parser.add_argument('--pie_value_col', type=str, help=\"Value column for pie chart\")\n",
    "    parser.add_argument('--pie_explode_index', type=int, default=0, help=\"Index of pie slice to explode\")\n",
    "    \n",
    "    parser.add_argument('--scatter_x_col', type=str, help=\"X-axis column for scatter plot\")\n",
    "    parser.add_argument('--scatter_y_col', type=str, help=\"Y-axis column for scatter plot\")\n",
    "    parser.add_argument('--scatter_hue_col', type=str, help=\"Hue column for scatter plot\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    original_df = load_data(args.file_path)\n",
    "    if original_df is None:\n",
    "        return\n",
    "\n",
    "    # Prepare data for analysis\n",
    "    cleaned_df = prepare_data(original_df)    \n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = calculate_summary_statistics(cleaned_df)\n",
    "    print(summary)\n",
    "\n",
    "    # Data filtering\n",
    "    if args.filter_column and args.filter_value:\n",
    "        filtered_df = filter_data(cleaned_df, args.filter_column, args.filter_value)\n",
    "        # Save the filtered data\n",
    "        save_processed_data(filtered_df, args.output_path)\n",
    "    else:\n",
    "        filtered_df = cleaned_df\n",
    "    \n",
    "    # Visualizations\n",
    "    if args.histogram_col:\n",
    "        plot_histogram(cleaned_df, args.histogram_col)\n",
    "    if args.bar_category_col and args.bar_value_col:\n",
    "        plot_bar_chart(cleaned_df, args.bar_category_col, args.bar_value_col)\n",
    "    if args.pie_category_col and args.pie_value_col:\n",
    "        plot_pie_chart(cleaned_df, args.pie_category_col, args.pie_value_col, args.pie_explode_index)\n",
    "    if args.scatter_x_col and args.scatter_y_col:\n",
    "        plot_scatter(cleaned_df, args.scatter_x_col, args.scatter_y_col, args.scatter_hue_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b603954-7054-4284-a87d-3de4ef203d49",
   "metadata": {},
   "source": [
    "# 10. DOCUMENTATIOM\n",
    "Hereâ€™s a brief document outline:\n",
    "## Overview\n",
    "This script reads a dataset (in CSV format), processes the data, performs operations such as cleaning the data,calculating summary statistics, filtering data, generating visualizations, and saving the processed data to a new file.\n",
    "\n",
    "## Dependencies\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- seaborn\n",
    "\n",
    "Install these dependencies via PIP:\n",
    "```pip install pandas numpy matplotlib seaborn```\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Place your dataset CSV file in the same directory as the script.\n",
    "2. Modify the `file_path` and `output_path` in the script to match your dataset and desired output file name.\n",
    "3. Run the script from the command line and specify the arguments. For example:\n",
    "\n",
    "    ```bash\n",
    "    python data_processing.py input_file.csv output_file.csv\n",
    "    ```\n",
    "\n",
    "\n",
    "    Or you can run the script with optional arguments which cover a more in depth analysis with this command:\n",
    "\n",
    "    ```bash\n",
    "    python data_processing.py input_file.csv output_file.csv --filter_column Month --filter_value 4 --histogram_col Sales --bar_category_col Product --bar_value_col Sales --pie_category_col City --pie_value_col Sales --scatter_x_col Price Each --scatter_y_col Sales --scatter_hue_col Product\n",
    "    ```\n",
    "\n",
    "    This command will:\n",
    "    - Load `input_file.csv`.\n",
    "    - Filter data where the `Month` column has a value of `4`.\n",
    "    - Plot a histogram of the `Sales` column.\n",
    "    - Create a bar chart of `Sales` by `Product`.\n",
    "    - Generate a pie chart of `Sales` by `City`.\n",
    "    - Create a scatter plot of `Sales` vs `Price Each` \n",
    "  \n",
    "   ### Argumets\n",
    "   - `file_path`: Path to the input CSV file.\n",
    "   - `output_path`: Path to save the processed CSV file.\n",
    "   - `--filter_column`: Column to filter on Month\n",
    "   - `--filter_value`: Value to filter by 4\n",
    "   - `--histogram_col`: Column for histogram\n",
    "   - `--bar_category_col`: Category column for bar chart\n",
    "   - `--bar_value_col`: Value column for bar chart\n",
    "   - `--pie_category_col`: Category column for pie chart.\n",
    "   - `--pie_value_col`: Value column for pie chart.\n",
    "   - `--pie_explode_index`: Index of pie slice to explode (defaut: 0).\n",
    "   - `--scatter_x_col`: X-axis column for scatter plot.\n",
    "   - `--scatter_y_col`: Y-axis column for scatter plot.\n",
    "   - `--scatter_hue_col`: Hue column for scatter plot.\n",
    "\n",
    "Feel free to customize the arguments based on the needs of your dataset and analysis.\n",
    "## Functions\n",
    "- **`load_data(file_path)`**: Loads the dataset from the provided file path.\n",
    "- **`prepare_data(original_df)`**: Cleans and prepares data for analysis.\n",
    "- **`calculate_summary_statistics(df)`**: Computes summary statistics.\n",
    "- **`filter_data(df, column, value)`**: Filters the data based on specific criteria.\n",
    "- **`plot_histogram(df, column)`**: Plots a histogram for a specified column.\n",
    "- **`plot_bar_chart(df, category_col, value_col)`**: Creates a bar chart of the specified category and value column.\n",
    "- **`plot_scatter(df, x_col, y_col)`**: Creates a scatter plot to show the relationship between two numerical values.\n",
    "- **`plot_pie_chart(df, category_col, value_col, explode_index=0)`**: Creates a pie chart.\n",
    "- **`save_processed_data(df, output_path)`**: Saves the processed data to a new CSV file.\n",
    "## Automation\n",
    "The script is adapted to handle different datasets by changing the file input path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ae77a-f861-4399-b1a2-fd1f396bd27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
